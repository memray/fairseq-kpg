#!/usr/bin/env bash

#/home/ubuntu/anaconda3/bin/conda config --set env_prompt '({name})'
#/home/ubuntu/anaconda3/bin/conda activate /home/ubuntu/efs/.conda/kp

export TOKENIZERS_PARALLELISM=false
export WANDB_NAME=bart-mag-labelled7m-frompretrained-lr5e5-step200k
export WANDB_API_KEY=72618587b1afa7c116440deb53224bd999919d0f
export CUDA_VISIBLE_DEVICES=0,1,2,3

cd /zfs1/hdaqing/rum20/kp/fairseq-kpg/fairseq_cli

# initial
CUDA_LAUNCH_BLOCKING=1;PYTHONUNBUFFERED=1;LD_LIBRARY_PATH=/usr/lib64:/ihome/crc/install/cuda/10.0.130/lib64/stubs:/ihome/crc/install/cuda/10.0.130/lib64;CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES;TOKENIZERS_PARALLELISM=false nohup /ihome/hdaqing/rum20/anaconda3/envs/kp/bin/python3.7 train.py /home/ubuntu/efs/rum20/data/kp/json/kp20k_train100k/train.json --dataset-type scipaper --save-dir /zfs1/hdaqing/rum20/kp/transfer_exps/kp_mag/bart-mag_labelled_200k-kp20k-lr1e5-step/ckpts --disable-validation --task keyphrasification --max-source-length 512 --max-target-length 64 --max-target-phrases 16 --kp-concat-type pres_abs --arch bart_large --restore-file /zfs1/hdaqing/rum20/kp/transfer_exps/kp_mag/bart-mag-labelled7m-frompretrained-lr5e5-step200k/ckpts/checkpoint_step_200000.pt --bpe hf_pretrained_bpe --bpe-vocab /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp/vocab.json --bpe-merges /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp/merges.txt --dict-path /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp/dict.txt --bpe-dropout 0.1 --ddp-backend=no_c10d --criterion label_smoothed_cross_entropy --share-all-embeddings --layernorm-embedding --share-all-embeddings --share-decoder-input-output-embed --reset-optimizer --reset-dataloader --reset-meters --required-batch-size-multiple 1 --optimizer adam --adam-betas "(0.9,0.999)" --adam-eps 1e-06 --weight-decay 0.01 --clip-norm 0.5 --lr 1e-5 --lr-scheduler polynomial_decay --label-smoothing 0.1 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 --log-format simple --log-interval 100 --fixed-validation-seed 7 --max-tokens 1920 --update-freq 2 --save-interval-updates 5000 --warmup-updates 10000 --max-update 100000 --total-num-update 100000 --num-workers 8 --find-unused-parameters --memory-efficient-fp16 --ddp-backend=no_c10d --wandb-project transfer_kp_mag > /zfs1/hdaqing/rum20/kp/transfer_exps/kp_mag/bart-mag_labelled_200k-kp20k-lr1e5-step/train.nohup.out &

# resume
#CUDA_LAUNCH_BLOCKING=1;PYTHONUNBUFFERED=1;LD_LIBRARY_PATH=/usr/lib64:/ihome/crc/install/cuda/10.0.130/lib64/stubs:/ihome/crc/install/cuda/10.0.130/lib64;CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES;TOKENIZERS_PARALLELISM=false nohup /ihome/hdaqing/rum20/anaconda3/envs/kp/bin/python3.7 train.py /zfs1/hdaqing/rum20/kp/data/kp/oag_v1_cs_kp --dataset-type scipaper --save-dir /zfs1/hdaqing/rum20/kp/transfer_exps/kp_mag/bart-mag-labelled7m-frompretrained-lr5e5-step200k/ckpts --disable-validation --task keyphrasification --max-source-length 512 --max-target-length 64 --max-target-phrases 16 --kp-concat-type pres_abs --arch bart_large --restore-file checkpoint_last.pt --bpe hf_pretrained_bpe --bpe-vocab /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp/vocab.json --bpe-merges /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp/merges.txt --dict-path /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp/dict.txt --bpe-dropout 0.1 --ddp-backend=no_c10d --criterion label_smoothed_cross_entropy --share-all-embeddings --layernorm-embedding --share-all-embeddings --share-decoder-input-output-embed --required-batch-size-multiple 1 --optimizer adam --adam-betas "(0.9,0.999)" --adam-eps 1e-06 --weight-decay 0.01 --clip-norm 0.5 --lr 5e-5 --lr-scheduler polynomial_decay --label-smoothing 0.1 --dropout 0.1 --attention-dropout 0.1 --log-format simple --log-interval 10 --fixed-validation-seed 7 --max-tokens 640 --update-freq 5 --save-interval-updates 5000 --warmup-updates 10000 --max-update 200000 --total-num-update 200000 --num-workers 12 --find-unused-parameters --memory-efficient-fp16 --ddp-backend=no_c10d --wandb-project transfer_kp_mag > /zfs1/hdaqing/rum20/kp/transfer_exps/kp_mag/bart-mag-labelled7m-frompretrained-lr5e5-step200k/train.nohup.resume0.out &
